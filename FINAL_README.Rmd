---
title: "Predicting Attraction in Speed Dating, STAT 432 Final Project"
author: "Alan Yu NetId: ayu24"
date: "December 15, 2018"
output:
  html_document:
    theme: cosmo
    toc: true
---

##Introduction

In this analysis, I will attempt to predict if a person will get a date based off of various qualitative and quantitative factors while also trying to understand the preferences underlying the selection of potential partners. The final decision is important (if you get a date or not), but individual decision-making are important as well. I want to figure out what kinds of qualities, characteristics and other aspects of attraction will yield the best results. "Can a machine predict if you will get a date or not".

##Literature review

Today we live in a very fast paced world and with everything going on it is hard to focus on one thing for very long. We all want to do a lot of things or have a lot of goals but time is limited. For example, some people want to feel loved and be loved and aren't content with being alone so something like like speed dating can be helpful and efficient for busy people or people who want to simply go out of their comfort zone and interact with others. Speed dating is a formalized matchmaking process where single people can get together and meet potential new partners in a short amount of time. Since your interactions with your potential partner is limited by time, one must make a good first impression if he or she wants to move forward and receive contact information. 

This data set was created and generated by Raymond Fisman, Sheena S. Iyengar, Emir Kamenica and Itamar Simonson from the University of Columbia School of Business in 2002-2004. They used this data to for their research paper called, "Gender Differences in Mate Selection: Evidence From a Speed Dating Experiment." The original purpose of the research was to analyze gender differences in dating preferences. They wanted to study dating behavior using an experimental speed dating market and serve as a starting point in understanding the thought process of selecting a mate. Everyone participating in the speed dating experiment get matched into pairs; male and female. they have a short, 4 minute conversation to make a good first impression. After each interaction, participants were asked to fill out a scorecard about what they thought about their potential partner. After cycling around everyone, each participant are asked who they want to date. If both feel the same way then their is match, but if either person says no then their is no match. 

After conducting the experiment they found out that women cared more about intelligence and race than men did when looking for a partner whereas men cared more about physical appearance (Fisman 2006). Women also prefer men who grew up in affluent or wealthy neighborhoods while men did not have such preference. Next they studied the selectivity between women and men. In small groups, women are no more selective than males; both males and females tend to select partners of equal social value. Psychologically, this makes sense to an extend; male choice reflects women's limited time to reproduction ability, so men seek women who "signals" that ability. Females want to seek men who can aid in supporting their offspring. In this analysis, I will try to replicate the similar results for most of these conclusions.

A similar study was done in 2017, which was lead by Samantha Joel, Paul W. Eastwick and Eli J. Finkel from the University of Utah, University of California, Davis, and Northwestern University respectively. Titled "Is Romantic Desire Predictable? Machine Learning Applied to Initial Romantic Attraction". In their research, they were able to predict the overall tendency for someone to like and to be liked, but not if you people were a particular match for each other (Joel 2017). In the experiment, Dr. Joel gave speed daters a questionnaire about traits and preferences before starting, then after a series of 4 minute interactions, they were asked again to rate their interactions from the level of interest and sexual attraction for each person they met. Even with over 100 predictors, they did not find any predictors that could capture any meaningful variance in the data. In the end they concluded that there is still much to be done in order to understand romantic attraction and what makes two people "click".

Bilbliography

Fisman, Raymond, et al. "Gender differences in mate selection: Evidence from a speed dating experiment." The Quarterly Journal of Economics 121.2 (2006): 673-697.

Joel, Samantha "A Magic Formula to Predict Attraction Is More Elusive than Ever." UNews, University of Utah, 30 Aug. 2017, unews.utah.edu/a-magic-formula-to-predict-attraction-is-more-elusive-than-ever/.

#Summary Statistics and Data Visualizations

```{r, warning = FALSE, message = FALSE}
library(magrittr)
library(dplyr)
library(ggplot2)
library(GGally)
library(cowplot)
library(GGally)
library(treemap)
library(data.table)
library(corrplot)
library(caret)
library(randomForest)
library(e1071)
library(gbm)
library(glmnet)
library(MASS)
library(klaR)
library(qcc)
library(colorspace)
library(ModelMetrics)
library(kableExtra)
```

To begin our analysis, We have 2 defined groups: male and female. lets remove irrelevant columns or redundant columns based on the knowledge of the data. After spiting the data to male and female Let's split the data into groups who managed to get a date and did not get a date.

```{r dataset, echo=FALSE}
rawdata = read.csv("speed dating.csv", header = TRUE)
data = as.data.frame(rawdata)
genderSet = split(data, data$gender)
fmatchSet = split(genderSet$female, genderSet$female$match)
mmatchSet = split(genderSet$male, genderSet$male$match)
```

Since there are so many variables, it would be too lengthy to summarize all of them, so I will present the most interesting or relevant to this analysis.

```{r echo=FALSE}
ggplot(data, aes(x=as.factor(age),fill=gender)) + 
  geom_bar(data=subset(data,gender =="female")) + 
  geom_bar(data=subset(data,gender =="male"),aes(y=..count..*(-1))) + 
  scale_y_continuous(breaks=seq(-1000,1000,50),labels=abs(seq(-1000,1000,50))) + 
  coord_flip()


frace = genderSet$female["race"]
mrace = genderSet$male["race"]

frace = as.data.frame(frace)
mrace = as.data.frame(mrace)

ggplot(frace, aes(x = race)) + geom_bar(aes(fill = race), position = "dodge") + ggtitle("Male Race Distribution")
ggplot(mrace, aes(x = race)) + geom_bar(aes(fill = race), position = "dodge") + ggtitle("Female Race Distribution")
```

The bar plot shows European/Caucasian-Americans are over represented in this data so their preferences and values might dominate certain attribute statistics. Most people in the data set were in there 20s-30s.

```{r echo=FALSE}

ggparcoord(genderSet$male, columns = 17:22, groupColumn = 'match', scale = 'uniminmax', title = "What do you look for in a partner? (Male)") + theme(axis.text.x = element_text(angle = 90)) 

ggparcoord(genderSet$female, columns = 17:22, groupColumn = 'match', scale = 'uniminmax', title = "What do you look for in a partner? (Female)") + theme(axis.text.x = element_text(angle = 90))

```

It appears that males feel that ambition is not an important attribute when looking for a partner while the over attributes are more diverse (funny not so much). 

For females, most do not put importance on any one attribute, but women who placed higher importance on appearance were more likely to get a date.

```{r echo=FALSE}
ggparcoord(genderSet$male, columns = 23:27, groupColumn = 'match', scale = 'uniminmax', title = "Rate yourself (Male)")
ggparcoord(genderSet$female, columns = 23:27, groupColumn = 'match', scale = 'uniminmax', title = "Rate yourself (Female)")
```

This seems like people are undervaluing their own attributes but overall there does not seem to be anything discernable from these parallel coordinate plots. This shows there is no importance placed on any one attribute. Below we will use bar charts to make this more distinct.

```{r echo=FALSE}
par(mar=c(3, 15, 3, 1))
barplot(sapply(genderSet$female[,17:22], mean), las = 1, horiz = TRUE, col = "pink", main = "Average Importance (Female)")

par(mar=c(3, 15, 3, 1))
barplot(sapply(genderSet$male[,17:22], mean), las = 1, horiz = TRUE, col = "lightblue", main = "Average Importance (Male)")

par(mar=c(3, 15, 3, 1))
barplot(sapply(genderSet$female[,23:27], mean), las = 1, horiz = TRUE, col = "pink", main = "Average Rate Yourself (Female)")

par(mar=c(3, 15, 3, 1))
barplot(sapply(genderSet$male[,23:27], mean), las = 1, horiz = TRUE, col = "lightblue", main = "Average Rate Yourself (Male)")

par(mar=c(3, 15, 3, 1))
barplot(sapply(genderSet$female[,45:61], mean), las = 1, horiz = TRUE, col = "pink", main = "Average Rate your Hobbies (Female)")
par(mar=c(3, 15, 3, 1))
barplot(sapply(genderSet$male[,45:61], mean), las = 1, horiz = TRUE, col = "lightblue", main = "Average Rate your Hobbies (Male)")

```

Again not nothing special sticks out, other than males valuing appearance than any category while women value intelligence the most. The hobbies chart between the two gender have some slimilarities.

```{r echo=FALSE}
ffieldData = as.data.frame(table(genderSet$female$field))
colnames(ffieldData) = c("field", "count")
levels(ffieldData$field) = tolower(levels(ffieldData$field))

treemap(ffieldData, index = "field", vSize = "count", type = "index", palette = "Reds", title = "What's your occupation? (Female)", fontsize.title = 14)

mfieldData = as.data.frame(table(genderSet$male$field))
colnames(mfieldData) = c("field", "count")
levels(mfieldData$field) = tolower(levels(mfieldData$field))

treemap(mfieldData, index = "field", vSize = "count", type = "index", palette = "Blues", title = "What's your occupation? (Male)", fontsize.title = 14)
```

Here we can see that for males, business students were over represented in the speed dating experiment. The female major distribution is more diverse.

Here is a correlation matrix between the numeric variables (only upper triangular matrix is shown for brevity). though not particularly useful with so much variables. There does not seem to be any variables that are significantly correlated with one another.

```{r echo=FALSE}
numdata = data[, sapply(data, is.numeric)]

corrplot(cor(numdata, use = "complete.obs", method = "pearson"), type = "upper", order = "hclust", method="color")
```


How many got a chance at a date?

```{r echo=FALSE}

histogram(data$match)
tmatch = as.data.frame(cbind(table(genderSet$male$match), table(genderSet$female$match)))
colnames(tmatch) = c("Male", "Female")
tmatch
```

Less than 20% of participants got a date.

Both are very similar but there is a slightly higher chance to get a date if you are a female. But Overall it is difficult to discern any meaningful conclusion based on looking at the data since everyone has different tastes and expectations and those may change again whenever you meet someone different; some maybe more accepting of your preferences and some may not.

#Analysis

To classify who got a date or not, we could try to fit a least squares estimate (linear regression model) on this data but with so many variables it may easily under fit or over fit the data. The limitation of a least squares approach is that it cannot capture all of the possible noise in the data. Instead we will use supervised learning methods such as Logistic Regression, Ridge Regression, Lasso Regression, Penalized Logistic Regression, and Boosted Trees. These methods were chosen based on prior knowledge. We will also use an unsupervised learning method (PCA).

First we need to remove the insignificant independent variables for this analysis since we do not need to know the difference in attributes (any with "d_xxx" is not relevant). split the data into train and test sets. 
create train and test split (80% of the data to is used to train our model and 20% of the data is to test our model). A 5 fold cross validation will be used as a compromise between good variation and computational run time.
```{r}
select = dplyr::select
MFData = data %>% select(-field, -d_intelligence_o, -d_funny_o, -d_shared_interests_o, -d_attractive, -d_sincere, -d_intelligence, -d_funny, -d_ambition, -d_attractive_partner, -d_sincere_partner, -d_intelligence_partner, -d_funny_partner, -d_ambition_partner, -d_shared_interests_partner, -d_sports,-d_tvsports, -d_exercise, -d_dining, -d_museums, -d_art, -d_hiking, -d_gaming, -d_clubbing, -d_reading, -d_tv, -d_theater, -d_movies, -d_concerts, -d_music, -d_shopping, -d_yoga, -d_like, -d_guess_prob_liked, -d_ambitous_o, -decision)

MFData = as.data.frame(MFData)
MFData = cbind(MFData, with(MFData, model.matrix(~ race + 0)), with(MFData, model.matrix(~ gender + 0)))
MFData = MFData %>% select(-gender, -race, -shared_interests_partner, -like, -expected_num_interested_in_me, -gendermale, -raceOther) #removed dupicate/redundant variables and variables with no variation
table(MFData$match)

trainIndex = createDataPartition(MFData$match, p = .8, list = FALSE, times = 1)
cv_5 = trainControl(method = "cv", number = 5)

MFDataTrain = MFData[trainIndex, ]
y.train = MFData[trainIndex]

MFDataTest = MFData[-trainIndex, ]
y.test = MFData[-trainIndex]
```

```{r echo=FALSE}
dataMC = apply(MFData[, -49], 2, function(y) y - mean(y))
pca = prcomp(dataMC[, -49], center = TRUE, scale. = TRUE)
round(pca$sdev^2, 2)

pcs = data.frame(pca$x)
cov = round(pca$sdev^2/sum(pca$sdev^2)* 100, 2)
cov = data.frame(c(1:52), cov)
names(cov)[1] = 'PCs'
names(cov)[2] = 'Variance'

plot(pca, type = "lines", pch = 19, main = "Match PCA variance")
```

Immediately we see that the all the principle components do not capture the variance very well, so this aligns with conclusions made in Dr. Joel's speeding dating research.

Looking at the "Match PCA Variance" plot there is not distinct "elbow" point so we use a Pareto Chart to assist in picking the ideal number of principle components. However, it still looks like the first 2 principle components are explains the variation the best.


```{r echo=FALSE}
PCA = pca$sdev^2
names(PCA) = paste0('PC', cov$PCs)
pareto.chart(PCA)
```

```{r warning = FALSE, message = FALSE}
sum(cov$Variance[1:2]) # % explained variance
g = ggbiplot::ggbiplot(pca, choices = 1:2, obs.scale = 1, var.scale = 1, groups = factor(dataMC[,49]), ellipse = T, circle = T)
g+theme(legend.position='top') + scale_colour_manual(values=c('red','green'))
```

Red indicates no date and Green indicates yes to a date. There is no clear seperation being the two groups.
Even using PCA does not yield high explained variance so we will move on to modeling.


```{r}
#set-up the full model
MFDataTrain$match = as.factor(MFDataTrain$match)
full_model  = formula(match ~ .)

accuracy = function(actual, predicted) {
  mean(actual == predicted)
}
```

Simple logistic regression model with all the variables.

```{r}
MFDataTrain = MFData[trainIndex, ]
MFDataTest = MFData[-trainIndex, ]

log_model = glm(match ~., MFDataTrain, family = binomial)
summary(log_model)

phat = predict(log_model, newdata = MFDataTest, type = "response")

predtype = ifelse(phat > .5, '0', '1')
errorRate = mean(predtype != MFDataTest$match)
1 - errorRate
```

This model does not perform very well since the misclassification rate is almost 91% (~9% prediction accuracy). Looking at the coefficients the variables with a p-value less than 0.05 are: importance_same_religion, attractive_o, funny_o, ambitous_o, shared_interests_o, sincere, funny, attractive_partner, tvsports, art and guess_prob_liked. We can use these variables for our reduced model and see if accuracy improves.

```{r}
log_model2 = glm(match ~ importance_same_religion + attractive_o + funny_o + ambitous_o + shared_interests_o + sincere + funny + attractive_partner + tvsports + art + guess_prob_liked, data = MFDataTrain, family = binomial)

summary(log_model2)

phat = predict(log_model2, newdata = MFDataTest, type = "response")

predtype = ifelse(phat > .5, '0', '1')
errorRate = mean(predtype != MFDataTest$match)
1 - errorRate 
```

fit another reduced model

```{r}
log_model3 = glm(match ~ attractive_o + funny_o + ambitous_o + shared_interests_o + sincere + funny + attractive_partner + art + guess_prob_liked, data = MFDataTrain, family = binomial)

phat = predict(log_model3, newdata = MFDataTest, type = "response")

predtype = ifelse(phat > .5, '0', '1')
errorRate = mean(predtype != MFDataTest$match)
1 - errorRate
```

After running a couple of models the prediction rate seems to plateau at ~15% prediction rate, that is to say, this model can predict if you will get a date ~15% of the time.

Ridge and Lasso Regression

```{r warning = FALSE, message = FALSE}
MFDataTrain = MFData[trainIndex, ]
y.train = MFData$match[trainIndex]
MFDataTrain = data.matrix(MFDataTrain)
y.train = data.matrix(y.train)

MFDataTest = MFData[-trainIndex, ]
MFDataTest = data.matrix(MFDataTest)
y.test = MFData$match[-trainIndex]
y.test = data.matrix(y.test)

ridge_model = cv.glmnet(MFDataTrain, y.train, alpha = 0)
lasso_model = cv.glmnet(MFDataTrain, y.train, alpha = 1)

y_hat_ridge = predict(ridge_model, MFDataTest)
y_hat_lasso = predict(lasso_model, MFDataTest)


sum((y.test - y_hat_ridge)^2)
sum((y.test - y_hat_ridge)^2)

lambda_ridge = expand.grid(alpha = 0, lambda = c(ridge_model$lambda.min, ridge_model$lambda.1se))
train_ridge = train(MFDataTrain[,-49], MFDataTrain[,49], method = "glmnet", trControl = cv_5, tuneGrid = lambda_ridge)

lambda_lasso = expand.grid(alpha = 1, lambda = c(lasso_model$lambda.min, lasso_model$lambda.1se))
train_lasso = train(MFDataTrain[, -49], MFDataTrain[,49], method = "glmnet", trControl = cv_5,tuneGrid = lambda_lasso)

method = c("Lasso", "Ridge")
cv_acc = c(train_lasso$results$RMSE, train_ridge$results$RMSE)
sd_cv_acc = c(train_lasso$results$RMSESD, train_ridge$results$RMSESD)
param_value = c(train_lasso$results$lambda, train_ridge$results$lambda)

results = data.frame(method, param_value, cv_acc, sd_cv_acc)
colnames(results) = c("Method", "Parameter Value", "CV-5 Accuracy", "Standard Deviation")
kable_styling(kable(results, format = "html", digits = 3), full_width = FALSE)
```

Both ridge and lasso regression have the same sum of squares as well as the same accuracy, but Ridge has the has a slightly higher lambda value since we want a model with a bigger penalty to reduce the chance of over fitting. So Overall, Ridge regression has the "best of the worst" accuracy.

Boosted Tree Model with a large tuning grid with a small interaction.depth and small shrinkage values.

```{r}
MFDataTrain = MFData[trainIndex, ]
y.train = MFData[trainIndex]

MFDataTest = MFData[-trainIndex, ]
y.test = MFData[-trainIndex]
MFDataTrain$match = as.factor(MFDataTrain$match)
gbm_grid  = expand.grid(interaction.depth = c(1:3), n.trees = 1:3 * 500, 
                        shrinkage = c(0.001, 0.01, 0.1), n.minobsinnode = 10)
gbm_full  = train(full_model, data = MFDataTrain, method = "gbm", 
                  verbose = FALSE, trControl = cv_5, tuneGrid = gbm_grid)
plot(gbm_full)
gbm_full$bestTune
gbm_full$finalModel
accuracy(predict(gbm_full, MFDataTest), MFDataTest$match)
rmse(predict(gbm_full, MFDataTest), MFDataTest$match)
```


Below is a penalized logistic regression, we will use a tuneLength of 5 so caret will try 5 $\alpha$ values and some $\lambda$ values for each. 

```{r}
MFDataTrain$match = as.factor(MFDataTrain$match)
glmn_full  = train(full_model, data = MFDataTrain, method = "glmnet", trControl = cv_5, tuneLength = 5)

plot(glmn_full)
glmn_full$bestTune
accuracy(predict(glmn_full, MFDataTest), MFDataTest$match)
rmse(predict(glmn_full, MFDataTest), MFDataTest$match)
```


Comparing the accuracy and root mean squared error between the penalized logistic regression and the boosted tree model, the boosted tree model model has slightly better accuracy over the penalized logistic regression but the penalized logistic regression has a slightly smaller root mean squared error. Overall it looks like the boosted tree model is the "best" model so we will move forward with that.

```{r}
par(mar=c(3, 20, 3, 1))
#summary(gbm_full$finalModel, las = 1)

```

The most influential variables from the boosted tree model are: attractive_partner (how you rate the attractiveness of your partner), attractive_o (how attractive your partner thinks you are), shared_interest_o (how much shared interest did you guys have), funny_o (how funny your partner thought of you), funny_partner (how funny was your partner) and guess_prob_liked (how likely do you think your partner likes you). This result seems very sensible as having a good time and being around with someone that enjoys your presence will lead to an ideal outcome (getting a date). These variables applies to both genders. If you scored or rate yourself high in these parameters then the model will say you will get a date.

#Conclusion

In the end, the boosted tree model performed the best out of all models used in this analysis. 
Based on the model we went forward with the best way to get a date is to just be a likable person and take care of your appearance. If you are attractive, funny your chances of getting a date is higher. Unlike in the research done by Fisman, race was not determined to be a significant factor in this model. The potential pitfall of this analysis is that accuracies vary with each run of certain models (Ridge, Lasso and Boosted Tree) so I made sure to run them multiple times to ensure a consistant accuracy and reasonable output. Many variables were excluded due to redundancy but it could be possible a significant variable was removed. Runtime was considered in this analysis (particularly, tunelength was reduced from 10 because it was taking a very long time to complete). There could also be a possibility that the tuning parameters are not the most optimal solution. 

It should be noted that the subjects used in the data set were graduate students or professional degree students at Columbia University and also people who self-selected themselves to participate so there could be a selection bias. This also explains the distribution of the population chart from earlier. Therefore this data is not representative of the general population or an indicator of what attributes does society favor the most. This study was over a decade old which means society and technology were very different back then (this is what people thought about back then). Love is complicated, A relationship between two people is more than just a sum of a hundred variables, there is a unique interaction that happens when you meet someone; that can't be predicted. Everyone has their own unique attributes and values that it would be hard to predict effectively. This model could not capture all the possible "right things to have" because it ultimately depends on you and the person you are talking to think. Sometimes it just comes down to luck and being in the right place at the right time. Future work in this dataset could try using neural networks or a sophisticated random forest model that can accomidate more variables/parameters. Random forest and neural networks are both very compuationally intense and running these model in R would take a lot of memory and the run time would be very long. 